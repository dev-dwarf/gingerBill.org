<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>memory allocation theory odin c on gingerBill</title>
    <link>/tags/memory-allocation-theory-odin-c/</link>
    <description>Recent content in memory allocation theory odin c on gingerBill</description>
    <language>en-gb</language>
    <lastBuildDate>Tue, 30 Nov 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/memory-allocation-theory-odin-c/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Memory Allocation Strategies - Part 5</title>
      <link>/article/2021/11/30/memory-allocation-strategies-005/</link>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/article/2021/11/30/memory-allocation-strategies-005/</guid>
      <description>Free List Based Allocation In the previous article, we looked at the pool allocator, which splits the supplied backing buffer into chucks of equal size and keeps track of which of the chunks are free. Pool allocators are fast allocators that allow for out of order free in constant time O(1) whilst keeping very little fragmentation. The main restriction of a pool allocator is that every memory allocation must be of the same size.</description>
    </item>
    
    <item>
      <title>Memory Allocation Strategies - Part 4</title>
      <link>/article/2019/02/16/memory-allocation-strategies-004/</link>
      <pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/article/2019/02/16/memory-allocation-strategies-004/</guid>
      <description>Pool-Based Allocation In the previous article, we looked at the stack allocator, which was the natural evolution of the linear/arena allocator. In this article, I will cover the fixed-sized pool allocator.
A pool allocator is a bit different from the previous allocation strategies that I have covered. A pool splits the supplied backing buffer into chunks of equal size and keeps track of which of the chunks are free. When an allocation is wanted, a free chunk is given.</description>
    </item>
    
    <item>
      <title>Memory Allocation Strategies - Part 3</title>
      <link>/article/2019/02/15/memory-allocation-strategies-003/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/article/2019/02/15/memory-allocation-strategies-003/</guid>
      <description>Stack-Like (LIFO) Allocation In the previous article, we looked at the linear/arena allocator, which is the simplest of all memory allocators. In this article, I will cover the fixed-sized stack-like allocator. Throughout this article, I will refer to this allocator as a stack allocator.
Note: A stack-like allocator means that the allocator acts like a data structure following the last-in, first-out (LIFO) principle. This has nothing to do with the stack or the stack frame.</description>
    </item>
    
    <item>
      <title>Memory Allocation Strategies - Part 2</title>
      <link>/article/2019/02/08/memory-allocation-strategies-002/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/article/2019/02/08/memory-allocation-strategies-002/</guid>
      <description>Linear Allocation The first memory allocation strategy that I will cover is also one of the simplest ones: linear allocation. As the name suggests, memory is allocated linearly. Throughout this series, I will be using the concept of an allocator as a means to allocate this memory. A linear allocator, is also known by other names such as an Arena or Region-based allocator. In this article, I will refer to this allocator an a Arena.</description>
    </item>
    
    <item>
      <title>Memory Allocation Strategies - Part 1</title>
      <link>/article/2019/02/01/memory-allocation-strategies-001/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/article/2019/02/01/memory-allocation-strategies-001/</guid>
      <description>Memory allocation seems to be something many people struggle with. Many languages try to automatically handle memory for you using different strategies: garbage collection (GC), automatic reference counting (ARC), resource acquisition is initialization (RAII), and ownership semantics. However, trying to abstract away memory allocation comes at a higher cost than most people realize.
Most people are taught to think of memory in terms of the stack and the heap, where the stack is automatically grown for a procedure call, and the heap is some magical thing that you can use to get memory that needs to live longer than the stack.</description>
    </item>
    
  </channel>
</rss>
